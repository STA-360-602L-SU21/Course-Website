<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STA 360/602L: Module 5.1</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# STA 360/602L: Module 5.1
## Hierarchical normal models with constant variance: two groups
### Dr. Olanrewaju Michael Akande

---








## Motivation

- Sometimes, we may have a natural grouping in our data, for example 
  + students within schools,
  + patients within hospitals,
  + voters within counties or states,
  + biology data, where animals are followed within natural populations organized geographically and, in some cases, socially.

--

- For such grouped data, we may want to do inference across all the groups, for example, comparison of the group means.

--

- Ideally, we should do so in a way that takes advantage of the relationship between observations in the same group, but we should also look to borrow information across groups when possible.

--

- .hlight[Hierarchical modeling] provides a principled way to do so. 



---
## Bayes estimators and bias

- Recall the normal model:
.block[
.small[
`$$y_{i} | \mu, \sigma^2 \overset{iid}{\sim} \mathcal{N} \left( \mu, \sigma^2\right).$$`
]
]

--

- The MLE for the population mean `\(\mu\)` is just the sample mean `\(\bar{y}\)`.

--

- `\(\bar{y}\)` is unbiased for `\(\mu\)`. That is, for any data `\(y_i \overset{iid}{\sim} \mathcal{N} \left( \mu, \sigma^2\right)\)`, `\(\mathbb{E}[\bar{y}] = \mu\)`.

--

- However, recall that in the conjugate normal model with known variance for example, the posterior expectation is a **weighted average** of the prior mean and the sample mean.

--

- That is, the posterior mean is actually biased.



---
## Shrinkage

- Usually through the weighting of the sample data and prior, Bayes procedures have the tendency to pull the estimate of `\(\mu\)` toward the prior
mean.

--

- Of course, the magnitude of the pull depends on the sample size.

--

- This "pulling" phenomenon is referred to as .hlight[shrinkage].

--

- &lt;div class="question"&gt;
Why would we ever want to do this? Why not just stick with the MLE?
&lt;/div&gt;

--

- Well, in part, because shrinkage estimators are often "more accurate" in prediction problems -- i.e. they tend to do a
better job of predicting a future outcome or of recovering the actual parameter values. Remember variance-bias trade off!

--

- The fact that a biased estimator would do a better job in many prediction problems can be proven rigorously, and is referred to as .hlight[Stein's paradox].



---
## Modern relevance

- Stein's result implies, in particular, that the sample mean is an *inadmissible* estimator of the mean of a multivariate normal distribution in more than two dimensions -- i.e. there are other estimators that will come closer to the true value in expectation.

--

- In fact, these are Bayes point estimators (the posterior expectation of the parameter `\(\mu\)`).

--

- Most of what we do now in high-dimensional statistics is develop biased estimators that perform better than unbiased ones.

--

- Examples: lasso regression, ridge regression, various kinds of hierarchical Bayesian models, etc.

--

- So, here we will get a very basic introduction to .hlight[Bayesian hierarchical models], which provide a formal and coherent framework for constructing shrinkage estimators.



---
## Why hierarchical models?

- **Bayesian hierarchical models** is a sort of catch-all phrase for a large class of models that have several levels of conditional distributions making up the prior.

--

- Like simpler one-level priors, they also accomplish shrinkage. However, they are much more flexible.

--

- Why use them? Several reasons:

  + We may want to exploit more complex dependence structures.

--
  + We may have many parameters relative to the amount of data that we have, and want to borrow information in estimating them.
  
--
  + We may want to shrink toward something other than a simple prior mean/hyper-parameter.



---
## Comparing two groups

- Suppose we want to do inference on mean body mass index (BMI) for two groups (male or female).

--

- BMI is known to often follow a normal distribution, so let's assume the same here.

--

- We should expect some relationship between the mean BMI for the two groups. 

--

- We may also think the shape of the two distributions would be relatively the same (at least as a simplifying assumption for now). 

--

- Thus, a reasonable model might be
.block[
.small[
$$
`\begin{split}
y_{i,male}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\theta_m, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
y_{i,female}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\theta_f, \sigma^2\right); \ \ i = 1, \ldots, n_f.\\
\end{split}`
$$
]
]

  but with some relationship between `\(\theta_m\)` and `\(\theta_f\)`.


---
## Bayesian inference

- One parameterization  that can reflect some relationship between `\(\theta_m\)` and `\(\theta_f\)` is
.block[
.small[
$$
`\begin{split}
y_{i,male}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu + \delta, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
y_{i,female}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu - \delta, \sigma^2\right); \ \ i = 1, \ldots, n_f.\\
\end{split}`
$$
]
]

--

  where
    + `\(\theta_m = \mu + \delta\)` and `\(\theta_f = \mu - \delta\)`,
    
    + `\(\mu = \dfrac{\theta_m + \theta_f}{2}\)` is the average of the population means, and 
    
    + `\(2\delta = \theta_m - \theta_f\)` is the difference in population means.

---
## Bayesian inference

- Convenient prior: 
  + `\(\pi(\mu, \delta, \sigma^2) = \pi(\mu) \cdot \pi(\delta) \cdot \pi(\sigma^2)\)`, where
      - `\(\pi(\mu) = \mathcal{N}(\mu_0, \gamma_0^2)\)`,
      
      - `\(\pi(\delta) = \mathcal{N}(\delta_0, \tau_0^2)\)`, and
      
      - `\(\pi(\sigma^2) = \mathcal{IG}(\dfrac{\nu_0}{2}, \dfrac{\nu_0 \sigma_0^2}{2})\)`.
      

---
## Bayesian inference

- Note that we can rewrite
.block[
.small[
$$
`\begin{split}
y_{i,male}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu + \delta, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
y_{i,female}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu - \delta, \sigma^2\right); \ \ i = 1, \ldots, n_f\\
\end{split}`
$$
]
]

  as
.block[
.small[
$$
`\begin{split}
(y_{i,male} - \delta) &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
(y_{i,female} + \delta)  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu, \sigma^2\right); \ \ i = 1, \ldots, n_f\\
\end{split}`
$$
]
]

--

  or
.block[
.small[
$$
`\begin{split}
(y_{i,male} - \mu) &amp; \overset{iid}{\sim} \mathcal{N} \left(\delta, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
(-1) (y_{i,female} - \mu)  &amp; \overset{iid}{\sim} \mathcal{N} \left(\delta, \sigma^2\right); \ \ i = 1, \ldots, n_f.\\
\end{split}`
$$
]
]

  as needed, so we can leverage past results for the full conditionals.



---
## Full conditionals

- For the full conditionals we will derive here, we will take advantage of previous results from the regular univariate normal model.

--

- Recall that if we assume
.block[
.small[
`$$y_i \sim \mathcal{N}(\mu, \sigma^2), \ \ i=1, \ldots, n,$$`
]
]

--

  and set our priors to be
.block[
.small[
$$
`\begin{split}
\pi(\mu) &amp; = \mathcal{N}\left(\mu_0, \gamma_0^2\right).\\
\pi(\sigma^2) &amp; = \mathcal{IG}\left(\dfrac{\nu_0}{2}, \dfrac{\nu_0 \sigma_0^2}{2}\right),\\
\end{split}`
$$
]
]

--

  then we have
.block[
.small[
$$
`\begin{split}
\pi(\mu, \sigma^2 | Y) &amp; \boldsymbol{\propto} \left\{ \prod_{i=1}^{n} p(y_{i} | \mu, \sigma^2 ) \right\} \cdot \pi(\mu) \cdot \pi(\sigma^2) \\
\end{split}`
$$
]
]



---
## Full conditionals

- We have
.block[
.small[
$$
`\begin{split}
\pi(\mu | \sigma^2, Y) &amp; = \mathcal{N}\left(\mu_n, \gamma_n^2\right).\\
\end{split}`
$$
]
]

  where
.block[
.small[
$$
`\begin{split}
\gamma^2_n = \dfrac{1}{ \dfrac{n}{\sigma^2} + \dfrac{1}{\gamma_0^2} }; \ \ \ \ \ \ \ \ \mu_n &amp; = \gamma^2_n \left[ \dfrac{n}{\sigma^2} \bar{y} + \dfrac{1}{\gamma_0^2} \mu_0 \right],
\end{split}`
$$
]
]

--

- and
.block[
.small[
$$
`\begin{split}
\pi(\sigma^2 | \mu,Y) \boldsymbol{=} \mathcal{IG}\left(\dfrac{\nu_n}{2}, \dfrac{\nu_n\sigma_n^2}{2}\right),
\end{split}`
$$
]
]

  where
.block[
.small[
$$
`\begin{split}
\nu_n &amp; = \nu_0 + n; \ \ \ \ \ \ \ \sigma_n^2  = \dfrac{1}{\nu_n} \left[ \nu_0 \sigma_0^2 + \sum^n_{i=1} (y_i - \mu)^2 \right].\\
\end{split}`
$$
]
]





---
## Full conditionals

- With `\(\pi(\mu) = \mathcal{N}(\mu_0, \gamma_0^2)\)`, and
.block[
.small[
$$
`\begin{split}
(y_{i,male} - \delta) &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
(y_{i,female} + \delta)  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu, \sigma^2\right); \ \ i = 1, \ldots, n_f,\\
\end{split}`
$$
]
]

  we have
.block[
.small[
$$
`\begin{split}
\mu | Y, \delta, \sigma^2 &amp; \sim \mathcal{N}(\mu_n, \gamma_n^2), \ \ \ \text{where}\\
\\
\gamma_n^2 &amp; = \dfrac{1}{\dfrac{1}{\gamma_0^2} + \dfrac{n_m + n_f}{\sigma^2} }\\
\\
\mu_n &amp; = \gamma_n^2 \left[\dfrac{\mu_0}{\gamma_0^2} + \dfrac{ \sum\limits_{i=1}^{n_m} (y_{i,male} - \delta) + \sum\limits_{i=1}^{n_f} (y_{i,female} + \delta) }{\sigma^2} \right].\\
\end{split}`
$$
]
]


---
## Full conditionals

- With `\(\pi(\delta) = \mathcal{N}(\delta_0, \tau_0^2)\)`, and
.block[
.small[
$$
`\begin{split}
(y_{i,male} - \mu) &amp; \overset{iid}{\sim} \mathcal{N} \left(\delta, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
(-1) (y_{i,female} - \mu)  &amp; \overset{iid}{\sim} \mathcal{N} \left(\delta, \sigma^2\right); \ \ i = 1, \ldots, n_f,\\
\end{split}`
$$
]
]

  we have
.block[
.small[
$$
`\begin{split}
\delta | Y, \mu, \sigma^2 &amp; \sim \mathcal{N}(\delta_n, \tau_n^2), \ \ \ \text{where}\\
\\
\tau_n^2 &amp; = \dfrac{1}{\dfrac{1}{\tau_0^2} + \dfrac{n_m + n_f}{\sigma^2} }\\
\\
\delta_n &amp; = \tau_n^2 \left[\dfrac{\delta_0}{\tau_0^2} + \dfrac{\sum\limits_{i=1}^{n_m} (y_{i,male} - \mu) + (-1) \sum\limits_{i=1}^{n_f} (y_{i,female} - \mu) }{\sigma^2} \right].\\
\end{split}`
$$
]
]

---
## Full conditionals

- With `\(\pi(\sigma^2) = \mathcal{IG}(\dfrac{\nu_0}{2}, \dfrac{\nu_0 \sigma_0^2}{2})\)`, and
.block[
.small[
$$
`\begin{split}
y_{i,male}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu + \delta, \sigma^2\right); \ \ i = 1, \ldots, n_m;\\
y_{i,female}  &amp; \overset{iid}{\sim} \mathcal{N} \left(\mu - \delta, \sigma^2\right); \ \ i = 1, \ldots, n_f\\
\end{split}`
$$
]
]

  we have
.block[
.small[
$$
`\begin{split}
\sigma^2 | Y, \mu, \delta &amp; \sim \mathcal{IG}(\dfrac{\nu_n}{2}, \dfrac{\nu_n \sigma_n^2}{2}), \ \ \ \text{where}\\
\\
\nu_n &amp; = \nu_0 + n_m + n_f\\
\\
\sigma_n^2 &amp; = \dfrac{1}{\nu_n} \left[\nu_0\sigma_0^2 + \sum\limits_{i=1}^{n_m} (y_{i,male} - [\mu + \delta])^2 + \sum\limits_{i=1}^{n_f} (y_{i,female} - [\mu - \delta])^2 \right].\\
\end{split}`
$$
]
]

--

- We will use write a Gibbs sampler for this model and fit the model to real data in the next module.


---

class: center, middle

# What's next? 

### Move on to the readings for the next module!




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
